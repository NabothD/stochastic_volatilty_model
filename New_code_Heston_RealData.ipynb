{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a857ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.12.8)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Naboth Dirirsa/OneDrive - Imperial College London/Year 4/FYP/FYP Coding/.venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, invgamma\n",
    "from filterpy.monte_carlo import systematic_resample\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "def refined_resample(V_tilde, W_tilde):\n",
    "\n",
    "\n",
    "    # 1) Sort the particles and their weights in ascending order by V\n",
    "    sort_indices = np.argsort(V_tilde)\n",
    "    V_sorted = np.array(V_tilde)[sort_indices]\n",
    "    W_sorted = np.array(W_tilde)[sort_indices]\n",
    "    N = len(V_sorted)\n",
    "\n",
    "    # Edge cases\n",
    "    if N == 1:\n",
    "        return V_sorted.copy()  # trivial single-particle case\n",
    "\n",
    "\n",
    "    partial_sum = np.zeros(N+1)\n",
    "    for k in range(N):\n",
    "        partial_sum[k+1] = partial_sum[k] + W_sorted[k]\n",
    "\n",
    "\n",
    "    cdf_left = np.zeros(N-1)\n",
    "    cdf_right = np.zeros(N-1)\n",
    "    slope = np.zeros(N-1)\n",
    "\n",
    "    # Interval j=0: from V_sorted[0] to V_sorted[1]\n",
    "    cdf_left[0] = 0.0\n",
    "    if N > 1:\n",
    "        cdf_right[0] = W_sorted[0] + 0.5 * W_sorted[1]\n",
    "        denom = (V_sorted[1] - V_sorted[0])\n",
    "        slope[0] = (cdf_right[0] - cdf_left[0]) / denom if denom > 1e-15 else 0.0\n",
    "\n",
    "    # Intervals j=1..N-2\n",
    "    for j in range(1, N-2):\n",
    "        cdf_left[j] = cdf_right[j-1]\n",
    "        # increment in cdf is 0.5 * W_sorted[j] + 0.5 * W_sorted[j+1]\n",
    "        inc = 0.5 * W_sorted[j] + 0.5 * W_sorted[j+1]\n",
    "        cdf_right[j] = cdf_left[j] + inc\n",
    "        denom = (V_sorted[j+1] - V_sorted[j])\n",
    "        slope[j] = (cdf_right[j] - cdf_left[j]) / denom if denom > 1e-15 else 0.0\n",
    "\n",
    "    # Last interval j=N-2: from V_sorted[N-2] to V_sorted[N-1]\n",
    "    if N > 1:\n",
    "        j = N-2\n",
    "        cdf_left[j] = 0.0 if j == 0 else cdf_right[j-1]\n",
    "        # increment in cdf is 0.5 * W_sorted[j] + W_sorted[j+1], if we follow eq. (72) strictly\n",
    "        # but we only have W_sorted up to index N-1 => W_sorted[N-1] is the last\n",
    "        # j+1 = N-1, so the increment is 0.5 * W_sorted[N-2] + W_sorted[N-1].\n",
    "        inc = 0.5 * W_sorted[j] + W_sorted[j+1]\n",
    "        # Ensure we don't exceed 1.0\n",
    "        cdf_right[j] = min(cdf_left[j] + inc, 1.0)\n",
    "        denom = (V_sorted[j+1] - V_sorted[j])\n",
    "        slope[j] = (cdf_right[j] - cdf_left[j]) / denom if denom > 1e-15 else 0.0\n",
    "\n",
    "    # Now we have piecewise intervals: [V_sorted[j], V_sorted[j+1]], cdf from cdf_left[j] to cdf_right[j].\n",
    "    # We'll invert that piecewise function for uniform random draws.\n",
    "\n",
    "    # 4) Inverse transform sampling\n",
    "    u = np.random.rand(N)  # draw N uniform random numbers in [0,1]\n",
    "    V_refined = np.zeros(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        # find the interval j where cdf_left[j] <= u[i] < cdf_right[j]\n",
    "        # We'll do a simple linear search for clarity. For large N, consider np.searchsorted.\n",
    "        ui = u[i]\n",
    "        # Special cases: if ui <= cdf_left[0], we clamp to V_sorted[0].\n",
    "        # If ui >= cdf_right[N-2], we clamp to V_sorted[N-1].\n",
    "        if ui <= cdf_left[0]:\n",
    "            V_refined[i] = V_sorted[0]\n",
    "            continue\n",
    "        # find j in [0..N-2]\n",
    "        j_found = None\n",
    "        for j in range(N-1):\n",
    "            if ui < cdf_right[j] or j == (N-2):\n",
    "                j_found = j\n",
    "                break\n",
    "        \n",
    "        if j_found is None:\n",
    "            # fallback if not found\n",
    "            V_refined[i] = V_sorted[-1]\n",
    "            continue\n",
    "        \n",
    "        # local interpolation\n",
    "        # cdf_left[j] + slope[j]*(v - V_sorted[j]) = u[i]\n",
    "        # => v = V_sorted[j] + (u[i] - cdf_left[j]) / slope[j]\n",
    "        if abs(slope[j_found]) < 1e-15:\n",
    "            # degenerate slope => all points are at V_sorted[j_found]\n",
    "            V_refined[i] = V_sorted[j_found]\n",
    "        else:\n",
    "            frac = (ui - cdf_left[j_found]) / slope[j_found]\n",
    "            V_refined[i] = V_sorted[j_found] + frac\n",
    "\n",
    "        # clamp if fraction tries to go beyond the interval\n",
    "        if V_refined[i] < V_sorted[j_found]:\n",
    "            V_refined[i] = V_sorted[j_found]\n",
    "        elif V_refined[i] > V_sorted[j_found+1]:\n",
    "            V_refined[i] = V_sorted[j_found+1]\n",
    "\n",
    "    return V_refined\n",
    "\n",
    "def heston_model_estimation(\n",
    "    n_samples, delta_t, maturity, prices, n_particles, \n",
    "    mu_0, kappa_0, theta_0, sigma_0, rho_0, \n",
    "    mu_eta_0, tau_eta_0, mu_beta_0, lambda_beta_0,\n",
    "    a_sigma_0, b_sigma_0, mu_psi_0, tau_psi_0, a_omega_0, b_omega_0\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform parameter estimation for the Heston model using Algorithm 1.\n",
    "    \"\"\"\n",
    "    n = len(prices) - 1  # number of intervals\n",
    "    \n",
    "\n",
    "    # Initialize arrays to store parameter estimates\n",
    "    mu_samples = np.zeros(n_samples)\n",
    "    kappa_samples = np.zeros(n_samples)\n",
    "    theta_samples = np.zeros(n_samples)\n",
    "    sigma_samples = np.zeros(n_samples)\n",
    "    rho_samples = np.zeros(n_samples)\n",
    "\n",
    "    # Current parameter guesses\n",
    "    mu = mu_0\n",
    "    kappa = kappa_0\n",
    "    theta = theta_0\n",
    "    sigma = sigma_0\n",
    "    rho = rho_0\n",
    "\n",
    "    # Define your prior distribution parameters\n",
    "    eta_prior_mean = mu_eta_0                 # scalar\n",
    "    eta_prior_precision = tau_eta_0           # scalar\n",
    "\n",
    "    # <-- Make beta_prior_mean a 1D array of shape (2,), not (2,1)\n",
    "    lambda_beta_0 = np.array(lambda_beta_0)\n",
    "    mu_beta_0 = np.array(mu_beta_0)\n",
    "    sigma_prior_a = a_sigma_0\n",
    "    sigma_prior_b = b_sigma_0\n",
    "\n",
    "    psi_prior_mean = mu_psi_0\n",
    "    psi_prior_precision = tau_psi_0\n",
    "    omega_prior_a = a_omega_0\n",
    "    omega_prior_b = b_omega_0\n",
    "    volatility_estimates_all = np.zeros((n_samples, n))\n",
    "    \n",
    "    MAX_PARAM = 1e4  # or choose a smaller bounding if you prefer\n",
    "\n",
    "\n",
    "    # Ratios R(t) = S(t+1)/S(t)\n",
    "    R = prices[1:] / prices[:-1]\n",
    "\n",
    "    V = [theta]*n_particles   \n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Particle storage: V[j] will be the current volatility value for particle j\n",
    "            # Initialize all particles V_j(0) = θ\n",
    "        # We'll store the final volatility estimates here\n",
    "        v_est = [0.0]*n\n",
    "\n",
    "        # Main time loop\n",
    "        for k in range(n):\n",
    "            # STEP 1: For each particle, propose a new volatility candidate\n",
    "            V_candidates = [0.0]*n_particles\n",
    "            weights      = [0.0]*n_particles\n",
    "            \n",
    "            for j in range(n_particles):\n",
    "                # (61) draw eps ~ N(0,1)\n",
    "                eps = np.random.normal(0,1)\n",
    "                \n",
    "                # (62) residual z_j(kΔt) using previous V[j] and R[k]\n",
    "                #     Typically: z_j = ( R(kΔt) - μΔt - 1 ) / sqrt( V_j((k-1)Δt)*( (k-1)Δt ) ), etc.\n",
    "                #     For simplicity, let's do a direct formula:\n",
    "                #     If k==0 we won't have a \"previous return,\" so assume R[0] means the first step\n",
    "                #     but that is just an example placeholder.\n",
    "                #     Adjust to your exact formula from Eq. (62):\n",
    "                z = (R[k] - mu*delta_t - 1.0) / math.sqrt( V[j]*delta_t )\n",
    "                \n",
    "                # (63) w_j(kΔt) = z_j ρ + ε_j sqrt(1 - ρ²)\n",
    "                w_ = z*rho + eps*math.sqrt(1.0 - rho**2)\n",
    "\n",
    "                # (64) new candidate\n",
    "                # V_j^tilde(kΔt) = V_j((k-1)Δt) + κ(θ - V_j((k-1)Δt)) Δt + σ_v sqrt(Δt V_j((k-1)Δt)) * w_\n",
    "                old_vol = V[j]\n",
    "                new_vol = old_vol + kappa*(theta - old_vol)*delta_t + sigma*math.sqrt(delta_t*old_vol)*w_\n",
    "                # Keep it nonnegative\n",
    "                new_vol = max(new_vol, 1e-12)\n",
    "\n",
    "                V_candidates[j] = new_vol\n",
    "\n",
    "            # STEP 2: Compute weights from likelihood (65) and normalize (66)\n",
    "            #  W_j(kΔt) = (1 / sqrt(2π V_j(kΔt) dt)) * exp( - ( (R(kΔt) - μdt -1)^2 ) / [2 V_j(kΔt) dt ] )\n",
    "            sum_w = 0.0\n",
    "            for j in range(n_particles):\n",
    "                vol_j = V_candidates[j]\n",
    "                # PDF for normal with variance = vol_j * dt\n",
    "                # If your model's normalizing constant is different, adapt accordingly.\n",
    "                var = vol_j * delta_t\n",
    "                std = math.sqrt(var)\n",
    "                x   = (R[k] - mu*delta_t - 1.0)\n",
    "                pdf = (1.0/(math.sqrt(2.0*math.pi)*std)) * math.exp(-0.5*(x/std)**2)\n",
    "                weights[j] = pdf\n",
    "                sum_w     += pdf\n",
    "\n",
    "            # Avoid divide-by-zero\n",
    "            if sum_w < 1e-15:  \n",
    "                # fallback, e.g. reset uniform weights\n",
    "                weights = [1.0/n_particles]*n_particles\n",
    "            else:\n",
    "                # normalize\n",
    "                weights = [w_/sum_w for w_ in weights]\n",
    "\n",
    "            # STEP 3: Pair up particles and weights, then RESAMPLE (67)–(73)\n",
    "            # A simple approach: \n",
    "            #   1) sort (V_candidates) in ascending order\n",
    "            #   2) build a piecewise‐linear CDF\n",
    "            #   3) sample from that CDF with inverse transform\n",
    "            \n",
    "            V = refined_resample(V_candidates, weights)\n",
    "            #---------------------------------------------------------------------------------------\n",
    "\n",
    "            # # 3A. Sort by ascending volatility\n",
    "            # pairs = sorted(zip(V_candidates, weights), key=lambda p: p[0])\n",
    "            # sorted_vols  = [p[0] for p in pairs]\n",
    "            # sorted_w     = [p[1] for p in pairs]\n",
    "\n",
    "            # # 3B. Build the cumulative distribution for the “continuous” approach\n",
    "            # #     The text shows a piecewise expression, but we can approximate\n",
    "            # #     by taking midpoints or “equal jumps” in between. For simplicity,\n",
    "            # #     do a standard discrete CDF:\n",
    "            # cdf = []\n",
    "            # running_sum = 0.0\n",
    "            # for j in range(n_particles):\n",
    "            #     running_sum += sorted_w[j]\n",
    "            #     cdf.append(running_sum)\n",
    "\n",
    "            # # 3C. Draw new “refined” particles via inverse‐transform sampling\n",
    "            # #     i.e., for each of N uniform draws U in [0,1], find the vol\n",
    "            # #     whose cdf bracket contains U. \n",
    "            # new_particles = []\n",
    "            # for _ in range(n_particles):\n",
    "            #     u = random.random()\n",
    "            #     # find index j s.t. cdf[j-1] < u <= cdf[j]\n",
    "            #     # or use a manual loop if we want to avoid libraries:\n",
    "            #     idx = 0\n",
    "            #     while idx < n_particles and cdf[idx] < u:\n",
    "            #         idx += 1\n",
    "            #     # to avoid out-of-bounds\n",
    "            #     idx = min(idx, n_particles-1)\n",
    "            #     new_particles.append(sorted_vols[idx])\n",
    "\n",
    "            # # Overwrite V with the newly refined particles\n",
    "            # V = new_particles\n",
    "            #---------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            # STEP 4: Estimate volatility at this step by the mean of refined particles (74)\n",
    "            v_est[k] = np.mean(V)\n",
    "\n",
    "        volatility_estimates_all[i, :] = v_est\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Make sure volatility_estimates is shape (n,)\n",
    "        # (That is already the case from np.zeros(n).)\n",
    "\n",
    "        #----------------------------------------------------------\n",
    "        # Step 2a: Estimate mu (drift) using eqns (13)–(23)\n",
    "        #     We'll treat \"eta = 1 + mu * dt\", etc.\n",
    "        #----------------------------------------------------------\n",
    "        # Summation term: sum(R / volatility_estimates) \n",
    "        # But watch out for shape mismatch: R is (n,), volatility_estimates is (n,).\n",
    "        # So R / volatility_estimates is (n,), then sum is scalar.\n",
    "\n",
    "        \n",
    "        # Build design matrix X: shape (n,2)\n",
    "        #   X = [ 1, volatility_estimate ] \n",
    "        #   R ~ X beta\n",
    "        # Make sure volatility_estimates is shape (n,)\n",
    "        \n",
    "\n",
    "        x_s = (1.0/np.sqrt(delta_t))*1.0/np.sqrt(v_est)\n",
    "        y_s = (1.0/np.sqrt(delta_t))*(R/np.sqrt(v_est))\n",
    "\n",
    "        ols =  np.dot(x_s.T, y_s)/(np.dot(x_s.T, x_s))\n",
    "\n",
    "\n",
    "        Tau_eta = np.dot(x_s.T, x_s) + eta_prior_precision\n",
    "\n",
    "        \n",
    "        # sum_term = np.sum(R / v_est)\n",
    "\n",
    "        # Posterior mean for eta\n",
    "        #   = ( tau_eta_0 * mu_eta_0 + sum(R/volatility) ) / ( tau_eta_0 + n )\n",
    "\n",
    "        eta_posterior_mean = (eta_prior_precision * eta_prior_mean + np.dot(x_s.T, x_s)*ols) / (Tau_eta)\n",
    "        \n",
    "        eta_sample = np.random.normal(eta_posterior_mean, 1.0/np.sqrt(Tau_eta))\n",
    "        mu = (eta_sample - 1.0)/delta_t\n",
    "\n",
    "        # Convert from eta to mu\n",
    "        # if we assume eta = 1 + mu*dt  => mu = (eta - 1)/dt\n",
    "        mu = np.clip(mu, -10., 10.)\n",
    "        mu_samples[i] = np.array(mu)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Step 2b: Estimate (kappa, theta) from eqns (24)–(44)\n",
    "        y_list, x1_list, x2_list = [], [], []\n",
    "\n",
    "        for k in range(1, n):\n",
    "            # Denominator for transformations: sqrt(∆t * v((k-1)*∆t))\n",
    "            denom = np.sqrt(delta_t * v_est[k-1])\n",
    "            if denom < 1e-15:\n",
    "                raise ValueError(f\"Volatility too close to zero at step {k-1}, cannot form regressors.\")\n",
    "\n",
    "            # yᵛₖ = v(k∆t) / sqrt(∆t * v((k-1)∆t))\n",
    "            y_list.append(v_est[k] / denom)\n",
    "\n",
    "            # x₁ₖ = 1 / sqrt(∆t * v((k-1)∆t))\n",
    "            x1_list.append(1.0 / denom)\n",
    "\n",
    "            # x₂ₖ = v((k-1)∆t) / sqrt(∆t * v((k-1)∆t)) = sqrt(v((k-1)∆t) / ∆t)\n",
    "            x2_list.append(v_est[k-1] / denom)\n",
    "\n",
    "        # Convert to NumPy arrays\n",
    "\n",
    "        beta_1 = kappa*theta*delta_t\n",
    "        beta_2 = 1-kappa*delta_t\n",
    "\n",
    "        Beta =np.array([beta_1,beta_2])\n",
    "        y_vec = np.array(y_list)                 # shape (n-1,)\n",
    "\n",
    "\n",
    "        x1_vec = np.array(x1_list)                # shape (n-1,)\n",
    "        x2_vec = np.array(x2_list)                # shape (n-1,)\n",
    "\n",
    "        # Xᵛ = [ x1   x2 ], shape (n-1, 2)\n",
    "        X_mat = np.column_stack((x1_vec, x2_vec))\n",
    "        # y_vec = X_mat@Beta.T \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ---------------------------------------------------------------\n",
    "        # 2. OLS estimate:  β̂ = (Xᵛᵀ Xᵛ)⁻¹ Xᵛᵀ yᵛ  (Eq. (38))\n",
    "        # ---------------------------------------------------------------\n",
    "        # Use pseudo-inverse to guard against ill-conditioned Xᵛᵀ Xᵛ\n",
    "        beta_hat_ols = np.linalg.pinv(np.dot(X_mat.T ,X_mat)) @ (X_mat.T @ y_vec)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # ---------------------------------------------------------------\n",
    "        # 3. Bayesian update (Eqns (36)–(39)) given prior β₀, Λ₀\n",
    "        #    Posterior precision: Λ^β = (Xᵛ)ᵀ Xᵛ + Λ₀\n",
    "        #    Posterior mean: µ^β = (Λ^β)⁻¹ [ Λ₀ β₀ + (Xᵛ)ᵀ yᵛ ]\n",
    "        # ---------------------------------------------------------------\n",
    "        \n",
    "        Lambda_beta = np.dot(X_mat.T, X_mat) + lambda_beta_0\n",
    "        rhs = lambda_beta_0 @ mu_beta_0 + (X_mat.T @ X_mat) @ beta_hat_ols\n",
    "        mu_beta = np.linalg.pinv(Lambda_beta) @ rhs\n",
    "\n",
    "        # ---------------------------------------------------------------\n",
    "        # (Optional) If you have or estimate σᵛ², you can sample from\n",
    "        # the posterior:\n",
    "        #   β ∼ Normal( mu_beta,  σᵛ² * (Λ^β)⁻¹ ).\n",
    "        # For demonstration we pick a dummy σᵛ² = 1e-4:\n",
    "        # ---------------------------------------------------------------\n",
    "        cov_beta = sigma**2 * np.linalg.pinv(Lambda_beta)\n",
    "\n",
    "        # Draw one sample from the posterior distribution:\n",
    "        beta_draw = np.random.multivariate_normal(mean=mu_beta, cov=cov_beta)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #----------------------------------------------------------\n",
    "        X = np.column_stack((np.ones(n), v_est))\n",
    "        \n",
    "        \n",
    "        kappa = (1.0 - beta_draw[1]) / delta_t\n",
    "        theta = beta_draw[0]/(kappa*delta_t)\n",
    "\n",
    "        kappa = np.clip(kappa, 1e-6, MAX_PARAM)\n",
    "        theta = np.clip(theta, 1e-6, MAX_PARAM)\n",
    "        kappa_samples[i] = np.array(kappa)\n",
    "        theta_samples[i] = np.array(theta)\n",
    "\n",
    "        #----------------------------------------------------------\n",
    "        # Step 2c: Sample sigma^2 from an Inverse Gamma\n",
    "        #----------------------------------------------------------\n",
    "        # Posterior scale\n",
    "        sigma_b = sigma_prior_b + 0.5 * (y_vec.T @ y_vec + mu_beta_0 @ lambda_beta_0 @ mu_beta_0  \n",
    "                                         - mu_beta.T @ Lambda_beta @ mu_beta )\n",
    "        # Posterior shape\n",
    "        sigma_a = sigma_prior_a + n / 2.0\n",
    "        # Draw from IG:\n",
    "        sigma_sq = invgamma.rvs(a=sigma_a, scale=sigma_b)\n",
    "        sigma = np.sqrt(sigma_sq)\n",
    "        sigma_samples[i] = np.array(sigma)\n",
    "\n",
    "        #----------------------------------------------------------\n",
    "        # Step 2d: Estimate rho from eqns (45)–(59)\n",
    "        # Here it's simplified. We do a single-sample approach:\n",
    "        #----------------------------------------------------------\n",
    "        # For demonstration, just do a small random walk around old rho\n",
    "        # or do a simple Normal prior for psi. \n",
    "\n",
    "\n",
    "        e1_rho = np.zeros(n)\n",
    "        e2_rho = np.zeros(n)\n",
    "\n",
    "        for t in range(n):\n",
    "            # eq. (45):\n",
    "            #   e1^rho(kΔt) = [R(kΔt) - (1 + μ·Δt)] / sqrt(Δt·v((k-1)Δt))\n",
    "            #   Here v((k-1)Δt) is volatility_estimates[t-1], but for t=0 we skip.\n",
    "            if t == 0:\n",
    "                # no prior v((−1)Δt)\n",
    "                e1_rho[t] = 0.0\n",
    "            else:\n",
    "                denom1 = np.sqrt(delta_t * np.clip(v_est[t-1],1e-12,None))\n",
    "                numerator1 = R[t] - (1.0 + mu*delta_t)\n",
    "                e1_rho[t] = numerator1 / denom1\n",
    "\n",
    "            # eq. (46):\n",
    "            #   e2^rho(kΔt) = [ v(kΔt) - v((k-1)Δt)  - κ(θ - v((k-1)Δt))Δt ]\n",
    "            #                 / sqrt(Δt·v((k-1)Δt))\n",
    "            if t == 0:\n",
    "                e2_rho[t] = 0.0\n",
    "            else:\n",
    "                dv = v_est[t] - v_est[t-1]\n",
    "                drift = kappa*(theta - v_est[t-1])*delta_t\n",
    "                denom2 = np.sqrt(delta_t * np.clip(v_est[t-1],1e-12,None))\n",
    "                e2_rho[t] = (dv - drift) / denom2\n",
    "\n",
    "        # 2) Form the matrix e^rho = [e1_rho, e2_rho], shape (n,2)\n",
    "        e_rho = np.column_stack((e1_rho, e2_rho))\n",
    "        \n",
    "\n",
    "        # 3) A^rho = (e^rho)' e^rho => shape (2,2)\n",
    "        A_rho = e_rho.T @ e_rho\n",
    "        \n",
    "        A11 = A_rho[0,0]\n",
    "        A12 = A_rho[0,1]\n",
    "        A22 = A_rho[1,1]\n",
    "\n",
    "        # 4) Draw omega from Inverse Gamma (Eqs. 56-57)\n",
    "        #    a^omega = a_omega_0 + n/2\n",
    "        #    b^omega = b_omega_0 + 0.5*( A22 - A12^2 / A11 )\n",
    "        a_omega = omega_prior_a + n/2.\n",
    "        b_omega = omega_prior_b + 0.5*( A22 - (A12**2)/A11 )\n",
    "        omega_draw = invgamma.rvs(a=a_omega, scale=b_omega)\n",
    "       \n",
    "        # 5) Draw psi from Normal (Eqs. 54-55)\n",
    "        #    tau^psi = A11 + tau_psi_0\n",
    "        #    mu^psi  = [ A12 + mu_psi_0 * tau_psi_0 ] / tau^psi\n",
    "        tau_psi = A11 + psi_prior_precision\n",
    "        mu_psi  = (A12 + psi_prior_mean * psi_prior_precision)/tau_psi\n",
    "        psi_draw = norm.rvs(loc=mu_psi, scale=math.sqrt(omega_draw / tau_psi))\n",
    "      \n",
    "        # 6) Finally, rho = psi / sqrt(psi^2 + omega)\n",
    "        new_rho = psi_draw / math.sqrt(psi_draw**2 + omega_draw)\n",
    "      \n",
    "\n",
    "        # clip if desired\n",
    "        rho = new_rho\n",
    "\n",
    "\n",
    "        rho_samples[i] = float(rho)\n",
    "\n",
    "\n",
    "    #--------------------------------------------\n",
    "    # Final estimates: average the MCMC draws\n",
    "    #--------------------------------------------\n",
    "    mu_hat = np.mean(mu_samples)\n",
    "    kappa_hat = np.mean(kappa_samples)\n",
    "    theta_hat = np.mean(theta_samples)\n",
    "    sigma_hat = np.mean(sigma_samples)\n",
    "    rho_hat = np.mean(rho_samples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        'mu': mu_hat,\n",
    "        'kappa': kappa_hat,\n",
    "        'theta': theta_hat,\n",
    "        'sigma': sigma_hat,\n",
    "        'rho': rho_hat,\n",
    "        'mu_samples': mu_samples,\n",
    "        'kappa_samples': kappa_samples,\n",
    "        'theta_samples': theta_samples,\n",
    "        'sigma_samples': sigma_samples,\n",
    "        'rho_samples': rho_samples.T,\n",
    "        'volatility_estimates_all': volatility_estimates_all\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_parameter_distributions(true_values, parameter_samples, parameter_names):\n",
    "\n",
    "    n_params = len(parameter_names)\n",
    "    fig, axes = plt.subplots(1, n_params, figsize=(15, 5), sharey=False)\n",
    "\n",
    "    for i, param in enumerate(parameter_names):\n",
    "        # Plot KDE for parameter samples\n",
    "        sns.kdeplot(parameter_samples[param], ax=axes[i], label='Estimate distribution', color='blue')\n",
    "        \n",
    "        # Add true value as a vertical line\n",
    "        if param in true_values:\n",
    "            axes[i].axvline(true_values[param], color='red', linestyle='--', label='True value')\n",
    "        \n",
    "        # Titles and labels\n",
    "        axes[i].set_title(f\"Parameter: {param}\")\n",
    "        axes[i].set_xlabel(\"Estimate\")\n",
    "        axes[i].set_ylabel(\"Empirical PDF\")\n",
    "        axes[i].legend()\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def load_prices_from_csv(filename):\n",
    "#     \"\"\"\n",
    "#     Load previously simulated data from CSV, return arrays of time, price, and variance.\n",
    "#     \"\"\"\n",
    "#     data = np.loadtxt(filename, delimiter=\",\", skiprows=1)  # skip the header\n",
    "#     prices = data[:, 0]\n",
    "#     variances = data[:, 1]\n",
    "#     return  prices, variances\n",
    "def load_prices_from_excel(filename):\n",
    "    df = pd.read_excel(filename)\n",
    "    prices = df['Price'].values\n",
    "    variances = df['ProxyVol'].values\n",
    "    return prices, variances\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Set your parameters\n",
    "    n_samples = 100\n",
    "    n_particles = 100\n",
    "\n",
    "    # Initial parameter values\n",
    "    T       = 1.0\n",
    "    \n",
    "\n",
    "    mu_0 = 0.1\n",
    "    kappa_0 =0.8\n",
    "    theta_0 = 0.03\n",
    "    sigma_0 = 0.011\n",
    "    rho_0 = -0.48\n",
    "\n",
    "    # t,prices, v_path = load_prices_from_csv(\"my_heston_euler.csv\")\n",
    "    # prices, v_path = load_prices_from_csv(\"my_heston.csv\")\n",
    "    prices, v_path = load_prices_from_excel(\"sp500_historical.xlsx\")\n",
    "    Nsteps  = len(prices)\n",
    "\n",
    "    delta_t = 1.0/len(prices)\n",
    "\n",
    "    T = 1.0\n",
    "\n",
    "    maturity = T\n",
    "\n",
    "    true_values = {\n",
    "    'mu': 0.1, 'kappa': 1, 'theta': 0.05, 'sigma': 0.01, 'rho': -0.5\n",
    "    }\n",
    "\n",
    "    # # Plot the synthetic path\n",
    "    # t_grid = np.linspace(0, T, Nsteps+1)\n",
    "    # fig, ax = plt.subplots(2, 1, figsize=(9,6), sharex=True)\n",
    "\n",
    "    # ax[0].plot(t_grid, prices, label=\"Synthetic Price\")\n",
    "    # ax[0].set_ylabel(\"Price\")\n",
    "    # ax[0].legend()\n",
    "\n",
    "    # ax[1].plot(t_grid, np.sqrt(v_path), label=\"Synthetic Vol (sqrt(v))\")\n",
    "    # ax[1].set_ylabel(\"Volatility\")\n",
    "    # ax[1].set_xlabel(\"Time (years)\")\n",
    "    # ax[1].legend()\n",
    "\n",
    "    # plt.suptitle(\"Heston Simulation with Known (True) Parameters\")\n",
    "    # plt.show()\n",
    "\n",
    "    # Priors\n",
    "    mu_eta_0 = 1.00125\n",
    "    tau_eta_0 = 1.0/math.sqrt(0.001**2)\n",
    "    mu_beta_0 = [35e-6, 0.998]        # <--- shape (2,)\n",
    "    lambda_beta_0 = [[10, 0], [0, 5]] # shape (2,2)\n",
    "    a_sigma_0 = 149\n",
    "    b_sigma_0 = 0.026\n",
    "    mu_psi_0 = -0.45\n",
    "    tau_psi_0 = 1.0/math.sqrt(0.25**2)\n",
    "    a_omega_0 = 1.03\n",
    "    b_omega_0 = 0.05\n",
    "\n",
    "\n",
    "    results = heston_model_estimation(\n",
    "        n_samples, delta_t, maturity, prices,\n",
    "        n_particles, mu_0, kappa_0, theta_0, sigma_0, rho_0, \n",
    "        mu_eta_0, tau_eta_0, mu_beta_0, lambda_beta_0, a_sigma_0, b_sigma_0, \n",
    "        mu_psi_0, tau_psi_0, a_omega_0, b_omega_0\n",
    "    )\n",
    "    # vols = results['volatility_estimates_all']  # shape (n_samples, n)\n",
    "    \n",
    "\n",
    "    print(\"Estimated Parameters:\")\n",
    "    print(f\"mu: {results['mu']}\")\n",
    "    print(f\"kappa: {results['kappa']}\")\n",
    "    print(f\"theta: {results['theta']}\")\n",
    "    print(f\"sigma: {results['sigma']}\")\n",
    "    print(f\"rho: {results['rho']}\")\n",
    "\n",
    "    def ensure_array(x):\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]  # unwrap\n",
    "        return np.array(x)\n",
    "    \n",
    "    v_path = ensure_array(v_path)\n",
    "\n",
    "    \n",
    "    vols = results['volatility_estimates_all']  # shape (n_samples, n)\n",
    "    final_volatility_path = ensure_array(vols[-1, :])         # last iteration\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(np.sqrt(final_volatility_path), label='Volatility (final iteration)')   # Estimated volatility \n",
    "    plt.plot(v_path, label=\"Synthetic Vol (sqrt(v))\")\n",
    "    # plt.plot(data.index, data[f\"RealizedVol_{window}\"], label=f\"{window}-day Realized Vol\") #real volatility\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Estimated Vol')\n",
    "    plt.legend()\n",
    "    plt.title('Particle-Filtered Volatility (final MCMC iteration)')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    avg_volatility_path = ensure_array(vols.mean(axis=0))  # shape (n,)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(np.sqrt(avg_volatility_path), label='Volatility (mean over all iterations)')\n",
    "    plt.plot(v_path, label=\"Synthetic Vol (sqrt(v))\")\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Estimated Vol')\n",
    "    plt.legend()\n",
    "    plt.title('Mean PF Volatility Estimate Over MCMC')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "   \n",
    "    mu_estimates = ensure_array(results.get('mu_samples', []))\n",
    "    kappa_estimates = ensure_array(results.get('kappa_samples', []))\n",
    "    theta_estimates = ensure_array(results.get('theta_samples', []))\n",
    "    sigma_estimates = ensure_array(results.get('sigma_samples', []))\n",
    "    rho_estimates = results.get('rho_samples', [])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Truncate all to the same length\n",
    "    df = pd.DataFrame({\n",
    "        'mu': mu_estimates,\n",
    "        'kappa': kappa_estimates,\n",
    "        'theta': theta_estimates,\n",
    "        'sigma': sigma_estimates,\n",
    "        'rho': rho_estimates\n",
    "    })\n",
    "\n",
    "\n",
    "    df2 = pd.DataFrame({\n",
    "        'Real': v_path[: len(final_volatility_path)],\n",
    "        'Final it': np.sqrt(final_volatility_path),\n",
    "        'Averege' : np.sqrt(avg_volatility_path)\n",
    "    })\n",
    "\n",
    "    # Save to Excel\n",
    "    df.to_excel('Heston_parameter_samples.xlsx', index=False)\n",
    "    df2.to_excel('Heston_Vols.xlsx', index=False)\n",
    "    print(\"Saved trimmed samples to 'Heston_parameter_samples.xlsx'\")\n",
    "\n",
    "\n",
    "    parameter_samples = {\n",
    "        'mu': mu_estimates,\n",
    "        'kappa': kappa_estimates,\n",
    "        'theta': theta_estimates,\n",
    "        'sigma': sigma_estimates,\n",
    "        'rho': rho_estimates,\n",
    "    }\n",
    "\n",
    "    parameter_names = ['mu', 'kappa', 'theta', 'sigma', 'rho']\n",
    "\n",
    "    # Plot the distributions\n",
    "    plot_parameter_distributions(true_values, parameter_samples, parameter_names)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
